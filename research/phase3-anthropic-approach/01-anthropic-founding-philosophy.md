# Research Notes: Anthropic - A Different Path

## The Founding Story (2021)

### The Exodus
- Key OpenAI safety researchers leave
- Dario Amodei (VP of Research) and Daniela Amodei (VP of Operations)
- ~10 other researchers follow
- Different vision for AI development

### Core Founding Principles:
1. **AI Safety as Primary Goal**
   - Not safety as afterthought
   - Built into every decision
   - Research-first approach

2. **Public Benefit Corporation**
   - Legal structure prioritizes safety
   - Not just profit maximization
   - Long-term thinking

3. **Frontier Research**
   - Push capabilities responsibly
   - Understand AI deeply
   - Empirical approach to safety

## The Anthropic Thesis:

### "AI Safety via Debate"
Early research direction:
- Train AI systems to critique each other
- Use AI to supervise AI
- Scale oversight with capabilities

### Interpretability Focus
- Understanding what models learn
- Mechanistic interpretability research
- "Opening the black box"

### Constitutional AI Vision
- Rules and principles, not just examples
- Scalable oversight
- Value alignment through constitution

## Key Early Papers:

### 1. "Predictability and Surprise in Large Generative Models" (2022)
- Scaling laws for safety properties
- Emergence isn't magic
- Predictable unpredictability

### 2. "Training a Helpful and Harmless Assistant" (2022)
- Foundation for Claude
- Beyond RLHF techniques
- Helpful, Harmless, Honest (HHH)

### 3. "Constitutional AI: Harmlessness from AI Feedback" (2022)
- Revolutionary training method
- AI provides its own feedback
- Principles over preferences

## The Different Approach:

### vs OpenAI:
- **OpenAI**: Capabilities first, safety follows
- **Anthropic**: Safety research drives capabilities
- **OpenAI**: Rapid deployment
- **Anthropic**: Cautious release

### vs Google:
- **Google**: Academic + product hybrid
- **Anthropic**: Pure research focus initially
- **Google**: Many projects
- **Anthropic**: Focused on foundation models

## Funding Philosophy:
- $124M Series A (2021)
- $580M Series B (2022)
- $750M Series C (2023)
- Google, Salesforce, others
- Patient capital for long-term research

## The Research Culture:

### Empirical Safety
- Test everything
- Measure harmful outputs
- Red team extensively
- Iterate based on data

### Transparency
- Detailed model cards
- Safety evaluations public
- Research papers released
- Open about limitations

### Collaborative Competition
- Work with other labs
- Share safety findings
- Compete on capabilities
- Cooperate on safety

## Early Claude Development:

### Claude 1.0 (Internal)
- Testing constitutional AI
- Extensive red teaming
- Learning from failures
- Not rushed to market

### Different Metrics
- Not just capability benchmarks
- Safety metrics equally important
- User trust paramount
- Long conversations prioritized

## The Technical Philosophy:

### 1. Simplicity Where Possible
- Avoid unnecessary complexity
- Clear training objectives
- Interpretable methods

### 2. Scale Thoughtfully
- Each scale-up planned
- Safety checks at each level
- No reckless scaling

### 3. Human-Centric Design
- AI as assistant not replacement
- Respect user intent
- Acknowledge limitations

## Why This Matters for Claude Code:

### Safety-First Development
- Tools require extra care
- Autonomous actions = higher stakes
- Permission systems critical

### Research Depth
- Understand code reasoning deeply
- Not just pattern matching
- True comprehension goal

### User Trust
- Predictable behavior
- Clear boundaries
- Honest about capabilities

## The Path Forward:
Anthropic's approach suggested:
- AI can be both capable and safe
- Research methodology matters
- Values can be embedded
- Different path possible

This foundation would shape everything about Claude...